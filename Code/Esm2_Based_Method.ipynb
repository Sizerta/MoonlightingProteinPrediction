{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl0EW8xlWWta"
      },
      "source": [
        "# Datasets Guide üìä\n",
        "\n",
        "#### You can use .npy files provided in ./Data/Esm2  file or make it yourself using ./Data/Esm2/Esm2.ipynb notebook\n",
        "\n",
        "##### MP_esm2.npy is for Mpfit positive samples and nonMP_esm2.npy is for Mpfit Negative samples.\n",
        "##### posshirafkan_esm2.npy is Shirafkan positive samples and negshirafkan_esm2.npy is for Shrafkan negative samples.\n",
        "##### lastly clean_esm2.npy is for cleaned up dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2Tv4QhuWWtc"
      },
      "source": [
        "# Code guide ‚å®Ô∏è\n",
        "\n",
        "# (python üêç)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_W7dphvWWtc"
      },
      "source": [
        "##  üì• Data Import and üîÆ Model Building and Testing on Mpfit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9htlBlSWWtc"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "import numpy as np\n",
        "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from imblearn.over_sampling import SVMSMOTE\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, roc_curve, roc_auc_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "# Load the .npy files\n",
        "embeddings_class2 = np.load(\"..\\\\Data\\\\Esm2\\\\MP_esm2.npy\")\n",
        "embeddings_class1 = np.load(\n",
        "    \"..\\\\Data\\\\Esm2\\\\nonMP_esm2.npy\")\n",
        "\n",
        "# Stack the embeddings to create a single array\n",
        "X = np.vstack((embeddings_class1, embeddings_class2))\n",
        "\n",
        "# Create a label array (0 for class 1, 1 for class 2)\n",
        "y = np.concatenate((np.zeros(len(embeddings_class1)),\n",
        "                   np.ones(len(embeddings_class2))))\n",
        "\n",
        "\n",
        "\n",
        "classifiers = [\n",
        "    (\"Extra Trees Classifier\", ExtraTreesClassifier(\n",
        "        n_estimators=500, random_state=42)),\n",
        "    (\"SVC\", SVC(kernel='rbf', C=10, gamma='auto', probability=True, random_state=42)),\n",
        "    (\"MLP\", MLPClassifier(hidden_layer_sizes=(200, 45),  # 2 hidden layers with 50 and 20 neurons\n",
        "                          activation='relu',  # ReLU activation function\n",
        "                          solver='adam',  # Adam optimizer\n",
        "                          batch_size=128,  # Batch size for training\n",
        "                          max_iter=200,  # Maximum number of iterations\n",
        "                          random_state=42)),\n",
        "    (\"Random Forest Classifier\", RandomForestClassifier(\n",
        "        n_estimators=500, random_state=42)),\n",
        "    (\"KNN\", KNeighborsClassifier(n_neighbors=25)),\n",
        "    (\"NB\", ComplementNB()),\n",
        "    (\"GBD\", GradientBoostingClassifier(\n",
        "        n_estimators=500, learning_rate=0.1, random_state=42))\n",
        "    # Add more classifiers as needed\n",
        "]\n",
        "\n",
        "\n",
        "for name, clf in classifiers:\n",
        "    scores = []\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    f1_scores = []\n",
        "    aurocs = []  # Store AUROC scores\n",
        "    auprcs = []  # Store AUPRC scores\n",
        "    tprs = []  # Store TPRs for ROC\n",
        "    fprs = []  # Store FPRs for ROC\n",
        "\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    for i, (train_index, val_index) in enumerate(kf.split(X)):\n",
        "        X_train, X_val = X[train_index], X[val_index]\n",
        "        y_train, y_val = y[train_index], y[val_index]\n",
        "\n",
        "        # Apply Min-Max scaling separately to the training and validation data\n",
        "        scaler = MinMaxScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "        clf.fit(X_train_scaled, y_train)\n",
        "        y_pred = clf.predict(X_val_scaled)\n",
        "\n",
        "        scores.append(accuracy_score(y_val, y_pred))\n",
        "        precisions.append(precision_score(y_val, y_pred))\n",
        "        recalls.append(recall_score(y_val, y_pred))\n",
        "        f1_scores.append(f1_score(y_val, y_pred))\n",
        "\n",
        "        # Calculate AUROC for this fold\n",
        "        y_pred_proba = clf.predict_proba(X_val_scaled)[:, 1]\n",
        "        fpr, tpr, thresholds = roc_curve(y_val, y_pred_proba)\n",
        "        auroc = roc_auc_score(y_val, y_pred_proba)\n",
        "        aurocs.append(auroc)\n",
        "        tprs.append(tpr)\n",
        "        fprs.append(fpr)\n",
        "\n",
        "        # Calculate AUPRC for this fold\n",
        "        precision, recall, _ = precision_recall_curve(y_val, y_pred_proba)\n",
        "        auprc = average_precision_score(y_val, y_pred_proba)\n",
        "        auprcs.append(auprc)\n",
        "\n",
        "    print(f\"{name}:\")\n",
        "    for i in range(5):\n",
        "        print(f\"Fold {i+1}: Accuracy = {scores[i]:.3f}, Precision = {precisions[i]:.3f}, Recall = {recalls[i]:.3f}, \"\n",
        "              f\"F1 = {f1_scores[i]:.3f}, AUROC = {aurocs[i]:.3f}, AUPRC = {auprcs[i]:.3f}\")\n",
        "    print(f\"Mean Accuracy: {np.mean(scores):.3f}, Mean Precision: {np.mean(precisions):.3f}, \"\n",
        "          f\"Mean Recall: {np.mean(recalls):.3f}, Mean F1: {np.mean(f1_scores):.3f}, \"\n",
        "          f\"Mean AUROC: {np.mean(aurocs):.3f}, Mean AUPRC: {np.mean(auprcs):.3f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mTfUrCGWWtd"
      },
      "source": [
        "## Testing on Shirafkan and independent dataset.üéØ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUq3MWbKWWtd",
        "outputId": "ff0e6aad-6475-425c-9c25-9b5b3be69600"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extra Trees Classifier Test Results:\n",
            "Accuracy: 0.94, Precision: 1.00, Recall: 0.91, F1: 0.95, AUROC: 1.00, AUPRC: 1.00\n",
            "\n",
            "SVC Test Results:\n",
            "Accuracy: 0.83, Precision: 0.94, Recall: 0.77, F1: 0.85, AUROC: 0.90, AUPRC: 0.94\n",
            "\n",
            "MLP Test Results:\n",
            "Accuracy: 0.85, Precision: 0.98, Recall: 0.78, F1: 0.87, AUROC: 0.95, AUPRC: 0.97\n",
            "\n",
            "Random Forest Classifier Test Results:\n",
            "Accuracy: 0.94, Precision: 1.00, Recall: 0.90, F1: 0.95, AUROC: 0.99, AUPRC: 1.00\n",
            "\n",
            "KNN Test Results:\n",
            "Accuracy: 0.82, Precision: 0.94, Recall: 0.76, F1: 0.84, AUROC: 0.90, AUPRC: 0.93\n",
            "\n",
            "NB Test Results:\n",
            "Accuracy: 0.78, Precision: 0.91, Recall: 0.72, F1: 0.80, AUROC: 0.86, AUPRC: 0.91\n",
            "\n",
            "GBD Test Results:\n",
            "Accuracy: 0.94, Precision: 1.00, Recall: 0.91, F1: 0.95, AUROC: 0.98, AUPRC: 0.99\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
        "\n",
        "# Load test embeddings\n",
        "test_class1 = np.load(\"..\\\\Data\\\\Esm2\\\\negshirafkan_esm2.npy\")\n",
        "test_class2 = np.load(\"..\\Data\\\\Esm2\\\\posshirafkan_esm2.npy\")\n",
        "\n",
        "# Load training embeddings\n",
        "embeddings_class2 = np.load(\"..\\\\Data\\\\Esm2\\\\MP_esm2.npy\")\n",
        "embeddings_class1 = np.load(\"..\\\\Data\\\\Esm2\\\\nonMP_esm2.npy\")\n",
        "\n",
        "# Prepare training data\n",
        "X_train = np.vstack((embeddings_class1, embeddings_class2))\n",
        "y_train = np.concatenate((np.zeros(len(embeddings_class1)), np.ones(len(embeddings_class2))))\n",
        "\n",
        "# Prepare test data\n",
        "X_test = np.vstack((test_class1, test_class2))\n",
        "y_test = np.concatenate((np.zeros(len(test_class1)), np.ones(len(test_class2))))\n",
        "\n",
        "# Define classifiers\n",
        "classifiers = [\n",
        "    (\"Extra Trees Classifier\", ExtraTreesClassifier(n_estimators=500, random_state=42)),\n",
        "    (\"SVC\", SVC(kernel='rbf', C=10, gamma='auto', probability=True, random_state=42)),\n",
        "    (\"MLP\", MLPClassifier(hidden_layer_sizes=(200, 45), activation='relu', solver='adam', batch_size=128, max_iter=200, random_state=42)),\n",
        "    (\"Random Forest Classifier\", RandomForestClassifier(n_estimators=500, random_state=42)),\n",
        "    (\"KNN\", KNeighborsClassifier(n_neighbors=25)),\n",
        "    (\"NB\", ComplementNB()),\n",
        "    (\"GBD\", GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, random_state=42))\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "# Train and test each classifier\n",
        "for name, clf in classifiers:\n",
        "    # Train the model\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Test the model\n",
        "    y_test_pred = clf.predict(X_test)\n",
        "    y_test_proba = clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Evaluate test performance\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "    test_precision = precision_score(y_test, y_test_pred)\n",
        "    test_recall = recall_score(y_test, y_test_pred)\n",
        "    test_f1 = f1_score(y_test, y_test_pred)\n",
        "    test_auroc = roc_auc_score(y_test, y_test_proba)\n",
        "    test_auprc = average_precision_score(y_test, y_test_proba)\n",
        "\n",
        "    print(f\"{name} Test Results:\")\n",
        "    print(f\"Accuracy: {test_accuracy:.2f}, Precision: {test_precision:.2f}, Recall: {test_recall:.2f}, \"\n",
        "          f\"F1: {test_f1:.2f}, AUROC: {test_auroc:.2f}, AUPRC: {test_auprc:.2f}\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
