{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0kq2RqdElWe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "830ea3c1-2ce2-45d2-b818-876a0523cda6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fair-esm\n",
            "  Downloading fair_esm-2.0.0-py3-none-any.whl.metadata (37 kB)\n",
            "Downloading fair_esm-2.0.0-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fair-esm\n",
            "Successfully installed fair-esm-2.0.0\n"
          ]
        }
      ],
      "source": [
        "#ESM\n",
        "!pip install fair-esm\n",
        "import esm\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "def make_esm_representations(df,col, model):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    #df = pd.read_excel(input_file_name)\n",
        "    peptides_list = df[col].tolist()\n",
        "\n",
        "    esm_model, alphabet = model\n",
        "    esm_model = esm_model.to(device)\n",
        "    batch_converter = alphabet.get_batch_converter()\n",
        "\n",
        "    peptides_list = [(\"\", peptides_list[i]) for i in range(len(peptides_list))]\n",
        "    batch_labels, batch_strs, batch_tokens = batch_converter(peptides_list)\n",
        "    batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
        "\n",
        "    # Extract per-residue representations (on CPU or CUDA)\n",
        "    results = esm_model(batch_tokens.to(device), repr_layers=[6], return_contacts=True)\n",
        "    token_representations = results[\"representations\"][6]\n",
        "\n",
        "    # Generate per-sequence representations via averaging\n",
        "    sequence_representations = []\n",
        "    for i, tokens_len in enumerate(batch_lens):\n",
        "        sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
        "\n",
        "    sequence_representations = torch.stack(sequence_representations)\n",
        "    df = pd.DataFrame(sequence_representations.cpu().detach().numpy())\n",
        "    return df\n",
        "    #df.to_excel(output_file_name, index=False)\n",
        "    #print(f'{output_file_name} file made')\n",
        "\n",
        "def pretrained_model(dim):\n",
        "    assert dim in [5120,2560,1280,640,480,320]\n",
        "    if dim==5120:\n",
        "        return esm.pretrained.esm2_t48_15B_UR50D()\n",
        "    elif dim==2560:\n",
        "        return esm.pretrained.esm2_t36_3B_UR50D()\n",
        "    elif dim==1280:\n",
        "        return esm.pretrained.esm2_t33_650M_UR50D()\n",
        "    elif dim==640:\n",
        "        return esm.pretrained.esm2_t30_150M_UR50D()\n",
        "    elif dim==480:\n",
        "        return esm.pretrained.esm2_t12_35M_UR50D()\n",
        "    elif dim==320:\n",
        "        return esm.pretrained.esm2_t6_8M_UR50D()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/Mp.csv\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "ii_EcibRXEhU",
        "outputId": "3b9859be-a018-4925-fee5-5ff49857ef4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            Sequence\n",
            "0  MSIITDVYAREVLDSRGNPTLEVEVYTESGAFGRGMVPSGASTGEH...\n",
            "1  MAITKIHARSVYDSRGNPTVEVDVVTETGLHRAIVPSGASTGQHEA...\n",
            "2  MAREFSLEKTRNIGIMAHVDAGKTTTTERILYYTGKIHKIGETHEG...\n",
            "3  MSIITDVYAREVLDSRGNPTLEVEVYTESGAFGRGMVPSGASTGEH...\n",
            "4  MAKVIGIDLGTTNSCLAISENGQTKVIENAEGARTTPSVIAYLDGG...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_size = 1\n",
        "results2 = []\n",
        "c = 0\n",
        "for chunk in pd.read_csv(\"/content/Mp.csv\", chunksize=chunk_size):\n",
        "    # Apply make_esm_representations function to the chunk\n",
        "    chunk_results = make_esm_representations(chunk,'Sequence', model=pretrained_model(320))\n",
        "    results2.append(chunk_results)\n",
        "    print(c)\n",
        "    c+=1\n",
        "    # Clear memory (optional, depending on your RAM capacity)\n",
        "    del chunk, chunk_results\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pW5mnKiYE3AE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abb8e9e9-575e-40d9-ece1-b79cd80270d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_df = pd.concat(results2)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "result_array = result_df.to_numpy()\n",
        "np.save('MP_esm2V2.npy', result_array)\n",
        "\n",
        "print(result_df.head())"
      ],
      "metadata": {
        "id": "T7kNonirhyso",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2046e1fd-eee0-42ae-eed2-6e5b7e2802ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        0         1         2         3         4         5         6    \\\n",
            "0  0.083978  0.072194  0.087015  0.146480  0.179757  0.053801  0.040912   \n",
            "0 -0.027097  0.001796  0.092432  0.147086  0.209930  0.130138 -0.000399   \n",
            "0 -0.031078  0.021842  0.042063  0.197593  0.243343  0.038649  0.079137   \n",
            "0  0.054546  0.064043  0.085998  0.170648  0.168747  0.069392  0.043424   \n",
            "0 -0.148942  0.031161  0.098035  0.157785  0.199050  0.167786  0.168320   \n",
            "\n",
            "        7         8         9    ...       310       311       312       313  \\\n",
            "0 -0.168640  0.075406 -0.190192  ... -0.020650  0.115753 -0.105518 -0.085044   \n",
            "0 -0.096349 -0.005315 -0.077787  ... -0.054319  0.109403 -0.082497 -0.018667   \n",
            "0 -0.092343 -0.057382 -0.059526  ...  0.042319  0.065393 -0.130362  0.137950   \n",
            "0 -0.181351  0.070163 -0.186088  ... -0.040261  0.115168 -0.104292 -0.082216   \n",
            "0 -0.053220 -0.139958 -0.062488  ...  0.136002 -0.020356 -0.067747  0.237618   \n",
            "\n",
            "        314       315       316       317       318       319  \n",
            "0  0.178561 -0.250703 -0.145796  0.093019  0.094352 -0.034155  \n",
            "0  0.105821 -0.282586 -0.153507  0.151567 -0.017981  0.010151  \n",
            "0  0.169453 -0.169591  0.014166  0.143301  0.044441 -0.043169  \n",
            "0  0.166562 -0.272091 -0.163105  0.086255  0.093729 -0.020515  \n",
            "0  0.025977 -0.359620 -0.020377  0.106588  0.037345 -0.071992  \n",
            "\n",
            "[5 rows x 320 columns]\n"
          ]
        }
      ]
    }
  ]
}